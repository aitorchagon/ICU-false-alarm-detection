{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "### %load_ext autoreload\n",
    "\n",
    "#!pip install --upgrade scipy \n",
    "\n",
    "#we import all the necessary packages\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "!pip install biosppy\n",
    "!pip install pyhrv\n",
    "!pip install neurokit2\n",
    "!pip install hrv\n",
    "!pip install heartpy\n",
    "!pip install neurokit\n",
    "\n",
    "import heartpy\n",
    "import biosppy\n",
    "import neurokit2 as nk\n",
    "import hrv\n",
    "import pyhrv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing part**   \n",
    "\n",
    "We aim to obtain all the signals. We have 550 signals associated to training and 250 associated to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/kaggle/input/physiological-signals-processing-challenge-2021/data_challenge/training/'\n",
    "\n",
    "lista_nombres = []\n",
    "for i in range(1, 551):\n",
    "    pat_filename = '{}.hea'.format(i)\n",
    "    lista_nombres.append(pat_filename)\n",
    "lista_señales = []\n",
    "for elem in lista_nombres:\n",
    "    pat = loadmat(os.path.join(path, elem[:-4] + '.mat')) # reading the signals' file.\n",
    "    lista_señales.append(pat)\n",
    "\n",
    "for elem in lista_señales:\n",
    "    elem = sc.signal.resample(elem['val'], 125) #resampling to 125 Hz.\n",
    "\n",
    "t = np.arange(0, 75000)/125\n",
    "plt.figure()\n",
    "plt.subplot(221)\n",
    "plt.plot(t, lista_señales[1]['val'][0,:75000])\n",
    "plt.title('Derivación II')\n",
    "plt.xlim(0, 50)\n",
    "plt.subplot(222)\n",
    "plt.plot(t, lista_señales[1]['val'][1,:75000])\n",
    "plt.title('Derivación V')\n",
    "plt.xlim(0, 50)\n",
    "plt.subplot(223)\n",
    "plt.plot(t, lista_señales[1]['val'][2, :75000])\n",
    "plt.title('PLETH')\n",
    "plt.xlim(0, 50)\n",
    "plt.subplot(224)\n",
    "plt.plot(t, lista_señales[1]['val'][3, :75000])\n",
    "plt.title('ABP')\n",
    "plt.xlim(0, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply a low-pass filter (IIR notch) at 50 Hz to remove streamline voltage.\n",
    "b, a= sc.signal.iirnotch(38, fs = 125, Q = 30)\n",
    "# We apply a pass-band filter at 0.5-40Hz to remove baseline wander (and EMG noise)\n",
    "c = sc.signal.firwin(64, [0.5, 40], pass_zero = False, fs=125, window='hamming')\n",
    "\n",
    "lista_filtrada_II = []\n",
    "lista_filtrada_V = []\n",
    "lista_filtrada_ABP = []\n",
    "lista_filtrada_PLETH = []\n",
    "for i in range (551):\n",
    "    try: #removing linear tendency and improving peaks, avoiding clipping on our signals.\n",
    "        elem1 = sc.signal.filtfilt(b, a, lista_señales[i]['val'][0, :75000]) \n",
    "        elem1 = sc.signal.filtfilt(c, 1, elem1)\n",
    "        elem1[np.isnan(elem1)] = 0.00001\n",
    "        elem1 = sc.signal.detrend(elem1)\n",
    "        elem1 = heartpy.enhance_peaks(elem1, iterations = 2)\n",
    "        lista_filtrada_II.append(elem1)\n",
    "        elem2 = sc.signal.filtfilt(b, a, lista_señales[i]['val'][1, :75000])\n",
    "        elem2 = sc.signal.filtfilt(c, 1, elem2)\n",
    "        elem2[np.isnan(elem2)] = 0.00001\n",
    "        elem2 = sc.signal.detrend(elem2)\n",
    "        elem2 = heartpy.enhance_peaks(elem2, iterations = 2)s\n",
    "        lista_filtrada_V.append(elem2)\n",
    "        elem3 = sc.signal.filtfilt(b, a, lista_señales[i]['val'][2, :75000])\n",
    "        elem3 = sc.signal.filtfilt(c,1, elem3)\n",
    "        elem3[np.isnan(elem3)] = 0.00001\n",
    "        elem3 = sc.signal.detrend(elem3)\n",
    "        elem3 = heartpy.enhance_peaks(elem3, iterations = 2)\n",
    "        lista_filtrada_PLETH.append(elem3)\n",
    "        elem4 = sc.signal.filtfilt(b, a, lista_señales[i]['val'][3, :75000])\n",
    "        elem4 = sc.signal.filtfilt(c, 1, elem4)\n",
    "        elem4[np.isnan(elem4)] = 0.00001\n",
    "        elem4 = sc.signal.detrend(elem4)\n",
    "        elem4 = heartpy.enhance_peaks(elem4, iterations = 2)\n",
    "        elem4 = lista_filtrada_ABP.append(elem4)\n",
    "    except IndexError:\n",
    "        continue\n",
    "    # IndexError appears as a result of the dataset; not all pà\n",
    "    # filtramos las señales. El IndexError aparece porque no siempre hay PLETH o ABP\n",
    "    #para cada uno de los pacientes, y porque no hay paciente 0. Por ello, simplemente \n",
    "    #pasamos de ello y seguimos ejecutando el bucle for.\n",
    "    #Tras representar las señales, no me queda muy claro\n",
    "    # que haya cambiado algo. Quizás ya se quitó ese ruido \n",
    "    # con anterioridad.\n",
    "\n",
    "t = np.arange(0, 75000)/125\n",
    "plt.figure()\n",
    "plt.title('Señales filtradas')\n",
    "plt.subplot(221)\n",
    "plt.plot(t, lista_filtrada_II[1])\n",
    "plt.title('Derivación II')\n",
    "plt.xlim(0, 50)\n",
    "plt.subplot(222)\n",
    "plt.plot(t, lista_filtrada_V[1])\n",
    "plt.title('Derivación V')\n",
    "plt.xlim(0, 50)\n",
    "plt.subplot(223)\n",
    "plt.plot(t, lista_filtrada_PLETH[1])\n",
    "plt.title('PLETH')\n",
    "plt.xlim(0, 50)\n",
    "plt.subplot(224)\n",
    "plt.plot(t, lista_filtrada_ABP[1])\n",
    "plt.title('ABP')\n",
    "plt.xlim(0, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aquí empezamos con el bloque de adquisición de características. Las características \n",
    "#que vamos a obtener son:\n",
    "#índice de pureza espectral\n",
    "#calidad de pleth y abp\n",
    "#media HR y decrecimiento PLETH Y ABP (OSc-ANFc-W)\n",
    "#maximo de HR intervals en ECG (AMM)\n",
    "#---------------------------------------------- Linear Discriminant Analysis/ECG--><3.5 Asistolia\n",
    "#media y mediana HR PLETH Y ABP (OSc-ANFc-W)\n",
    "#minimo HR en 5 latidos consecutivos (AMM)\n",
    "#----------------------------------------------media y mediana > 54bpm/ECG-->>40bpm\n",
    "#HR máximo averaged en ventanas de 3 segundos (OSc-ANFc-W) PLETH-ABP ---> <90bpm\n",
    "#HR mínimo averaged en ventanas de 3 segundos (OSc-ANFc-W) PLETH-ABP----> >60bpm\n",
    "#En ECG igual con SPI (uno averaged y el otro incremento máximo) ---> max(SPI) <0.25 & and (max(SPI) <0.36\n",
    "#----------------------------------------------------------------------max(SPI increase <0.012 and (max(SPI) <0.36 & max(SPI increase) <0.2\n",
    "#--------------------------------------------------\n",
    "#max averaged SPI en ventanas de 3 segundos <0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtenemos los picos R de las señales ECG y los intervalos RR\n",
    "rpeaks_II = []\n",
    "properties_II = []\n",
    "rpeaks_V= []\n",
    "properties_V = []\n",
    "rr_interval_V = []\n",
    "rr_interval_II = []\n",
    "for i in range(len(lista_filtrada_V)):\n",
    "    rpeaksV, _ = sc.signal.find_peaks(lista_filtrada_V[i], distance = 150)\n",
    "    rpeaksV = biosppy.signals.ecg.correct_rpeaks(lista_filtrada_V[i], rpeaks = rpeaksV, sampling_rate = 125, tol = 0.04)\n",
    "    rr_intervalV = np.diff(rpeaksV)/125 * 1000\n",
    "    rpeaks_V.append(rpeaksV['rpeaks'])\n",
    "    rr_interval_V.append(rr_intervalV)\n",
    "for i in range(len(lista_filtrada_II)):\n",
    "    rpeaksII, _ = sc.signal.find_peaks(lista_filtrada_II[i], distance = 150)\n",
    "    rpeaksII = biosppy.signals.ecg.correct_rpeaks(lista_filtrada_II[i], rpeaks = rpeaksII, sampling_rate = 125, tol = 0.04)\n",
    "    rpeaks_II.append(rpeaksII['rpeaks'])\n",
    "    rr_intervalII = np.diff(rpeaksII)/125 * 1000\n",
    "    rr_interval_II.append(rr_intervalII)\n",
    "for i in range(len(lista_filtrada_V)):\n",
    "    rpeaksV, propertiesV = sc.signal.find_peaks(lista_filtrada_V[i], distance = 150, prominence = 1, width = 20)\n",
    "    properties_V.append(propertiesV[\"width_heights\"])\n",
    "    properties_V.append(propertiesV[\"prominences\"])\n",
    "for i in range(len(lista_filtrada_II)):\n",
    "    rpeaksII, propertiesII = sc.signal.find_peaks(lista_filtrada_II[i], distance = 150, prominence = 1, width = 20)\n",
    "    properties_II.append(propertiesII[\"width_heights\"])\n",
    "    properties_II.append(propertiesII[\"prominences\"])\n",
    "#Detección de arritmias, en este caso fibrilación ventricular\n",
    "#rpeaksV, propertiesV = sc.signal.find_peaks(lista_filtrada_V[40], distance = 150, prominence = 1, width = 20)\n",
    "#anchura_pico = propertiesV['width_heights'] #te devuelve el ancho del pico\n",
    "#importante para arritmias por ejemplo donde, al haber más latidos, el pico R es más estrecho\n",
    "#prominencia = propertiesV['prominences'] #te devuelve cómo de alto es el pico\n",
    "#en relación con la línea de base.\n",
    "#Las podemos meter juntas o por separado, ver cómo funciona mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estadísticas principales: media, mediana, amplitud máxima, cuasivarianza, \n",
    "#desviación estándar, desviación absoluta, kurtosis, skewness\n",
    "mediaV = []\n",
    "medianaV = []\n",
    "amplitudmaxV = [] \n",
    "cuasivaV = []\n",
    "stdV = []\n",
    "stdabsV = [] \n",
    "kurtV = []\n",
    "skewV = []\n",
    "mediaII = [] \n",
    "medianaII = []\n",
    "amplitudmaxII = [] \n",
    "cuasivaII = []\n",
    "stdII = []\n",
    "stdabsII = [] \n",
    "kurtII = []\n",
    "skewII = []\n",
    "mediaPLETH = [] \n",
    "medianaPLETH = []\n",
    "amplitudmaxPLETH = [] \n",
    "cuasivaPLETH = []\n",
    "stdPLETH = []\n",
    "stdabsPLETH = [] \n",
    "kurtPLETH = []\n",
    "skewPLETH = []\n",
    "for i in range(len(lista_filtrada_V)):\n",
    "    a = biosppy.signals.tools.signal_stats(lista_filtrada_V[i])\n",
    "    mediaV.append(a[0])\n",
    "    medianaV.append(a[1])\n",
    "    amplitudmaxV.append(a[2])\n",
    "    cuasivaV.append(a[3])\n",
    "    stdV.append(a[4])\n",
    "    stdabsV.append(a[5])\n",
    "    kurtV.append(a[6])\n",
    "    skewV.append(a[7])\n",
    "for i in range(len(lista_filtrada_II)):\n",
    "    b = biosppy.signals.tools.signal_stats(lista_filtrada_II[i])\n",
    "    mediaII.append(b[0])\n",
    "    medianaII.append(b[1])\n",
    "    amplitudmaxII.append(b[2])\n",
    "    cuasivaII.append(b[3])\n",
    "    stdII.append(b[4])\n",
    "    stdabsII.append(b[5])\n",
    "    kurtII.append(b[6])\n",
    "    skewII.append(b[7])\n",
    "for i in range(len(lista_filtrada_PLETH)):\n",
    "    c = biosppy.signals.tools.signal_stats(lista_filtrada_PLETH[i])\n",
    "    mediaPLETH.append(c[0])\n",
    "    medianaPLETH.append(c[1])\n",
    "    amplitudmaxPLETH.append(c[2])\n",
    "    cuasivaPLETH.append(c[3])\n",
    "    stdPLETH.append(c[4])\n",
    "    stdabsPLETH.append(c[5])\n",
    "    kurtPLETH.append(c[6])\n",
    "    skewPLETH.append(c[7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos la autocorrelación\n",
    "autocorV = []\n",
    "autocorII = []\n",
    "autocorPLETH = []\n",
    "autocorABP = []\n",
    "for i in range(len(lista_filtrada_V)):\n",
    "    autocor_V = np.correlate(lista_filtrada_V[i], lista_filtrada_V[i])\n",
    "    autocorV.append(autocor_V)\n",
    "for i in range(len(lista_filtrada_II)):\n",
    "    autocor_II = np.correlate(lista_filtrada_II[i], lista_filtrada_II[i])\n",
    "    autocorII.append(autocor_II)\n",
    "for i in range(len(lista_filtrada_PLETH)):\n",
    "    autocor_PLETH = np.correlate(lista_filtrada_PLETH[i], lista_filtrada_PLETH[i])\n",
    "    autocorPLETH.append(autocor_PLETH)\n",
    "for i in range(len(lista_filtrada_ABP)):\n",
    "    autocor_ABP = np.correlate(lista_filtrada_ABP[i], lista_filtrada_ABP[i])\n",
    "    autocorABP.append(autocor_ABP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos ciertas características del PLETH. \n",
    "#De momento, no obtenemos características concretas de ABP.\n",
    "picos_sistolicos = []\n",
    "for i in range(len(lista_filtrada_PLETH)):\n",
    "    try: #Calculamos las muestras donde se encuentran los picos sistólicos\n",
    "        sistolic_peaks = nk.ppg_findpeaks(lista_filtrada_PLETH[i], sampling_rate = 125)  \n",
    "        picos_sistolicos.append(sistolic_peaks['PPG_Peaks'][0])\n",
    "        \n",
    "    except IndexError: #Probablemente salte por el mismo motivo que antes\n",
    "        #por no haber suficientes latidos (habrá una bradicardia)\n",
    "        #Salta en 4, 64, 71, 289, 336 y 434.\n",
    "        picos_sistolicos.append(0)\n",
    "#Calculamos la tasa cardiaca en PLETH\n",
    "heart_rate_PLETH = []\n",
    "for i in range(len(picos_sistolicos)):\n",
    "    tasa_cardiaca_ppg = nk.ppg_rate(picos_sistolicos[i], sampling_rate = 125)\n",
    "    heart_rate_PLETH.append(tasa_cardiaca_ppg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A continuación, nos disponemos a calcular características del propio ECG.\n",
    "#bpm instantáneo\n",
    "templates_II = []\n",
    "beats_II = []\n",
    "templates_V = []\n",
    "beats_V = []\n",
    "for i in range(len(lista_filtrada_II)):\n",
    "    templates, rpiks = biosppy.signals.ecg.extract_heartbeats(lista_filtrada_II[i], rpeaks_II[i], sampling_rate = 125)\n",
    "    templates_II.append(templates)\n",
    "    beats_II.append(rpiks)\n",
    "for i in range(len(lista_filtrada_V)):\n",
    "    templates, rpiks = biosppy.signals.ecg.extract_heartbeats(lista_filtrada_V[i], rpeaks_V[i], sampling_rate = 125)\n",
    "    templates_V.append(templates)\n",
    "    beats_V.append(rpiks)\n",
    "heart_rate_inst_II = []\n",
    "heart_rate_inst_V = []\n",
    "for i in range(len(beats_II)):\n",
    "    try:\n",
    "        index, heart_rate = biosppy.signals.tools.get_heart_rate(beats_II[i], sampling_rate = 125)\n",
    "        heart_rate_inst_II.append(heart_rate)\n",
    "    except ValueError: #Salta porque hay bradicardias probablemente\n",
    "        heart_rate_inst_II.append(50)\n",
    "for i in range(len(beats_V)):\n",
    "    try:\n",
    "        index, heart_rate = biosppy.signals.tools.get_heart_rate(beats_V[i], sampling_rate = 125)\n",
    "        heart_rate_inst_V.append(heart_rate)\n",
    "    except ValueError: #Salta porque hay bradicardias probablemente\n",
    "        heart_rate_inst_II.append(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sacamos todos los onsets ,offsets y el resto de picos aparte de R que conforman PQRS\n",
    "#ecg_delineate()\n",
    "tpeaks_V = []\n",
    "toffsets_V = []\n",
    "ppeaks_V = []\n",
    "ponsets_V = []\n",
    "tpeaks_II = []\n",
    "toffsets_II = []\n",
    "ppeaks_II = []\n",
    "ponsets_II = []\n",
    "for i in range(len(lista_filtrada_V)):\n",
    "    try:\n",
    "        proceso = nk.ecg_delineate (lista_filtrada_V[i], rpeaks = rpeaks_V, sampling_rate = 125, method = 'dwt') \n",
    "        tpeaks_V.append(proceso[0]['ECG_T_Peaks'])\n",
    "        toffsets_V.append(proceso[0]['ECG_T_Offsets'])\n",
    "        ppeaks_V.append(proceso[0]['ECG_P_Peaks'])\n",
    "        ponsets_V.append(proceso[0]['ECG_P_Onsets'])\n",
    "    except ValueError: #Taquicardia\n",
    "        tpeaks_V.append(0.0001)\n",
    "        toffsets_V.append(0.0001)\n",
    "        ppeaks_V.append(0.0001)\n",
    "        ponsets_V.append(0.0001)\n",
    "    except IndexError:\n",
    "        tpeaks_V.append(0.0001)\n",
    "        toffsets_V.append(0.0001)\n",
    "        ppeaks_V.append(0.0001)\n",
    "        ponsets_V.append(0.0001)\n",
    "for i in range(len(lista_filtrada_V)):\n",
    "    try:\n",
    "        proceso = nk.ecg_delineate (lista_filtrada_II[i], rpeaks = rpeaks_V, sampling_rate = 125, method = 'dwt') \n",
    "        tpeaks_II.append(proceso[0]['ECG_T_Peaks'])\n",
    "        toffsets_II.append(proceso[0]['ECG_T_Offsets'])\n",
    "        ppeaks_II.append(proceso[0]['ECG_P_Peaks'])\n",
    "        ponsets_II.append(proceso[0]['ECG_P_Onsets'])\n",
    "    except ValueError:#Taquicardias\n",
    "        tpeaks_II.append(0.0001)\n",
    "        toffsets_II.append(0.0001)\n",
    "        ppeaks_II.append(0.0001)\n",
    "        ponsets_II.append(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seguimos obteniendo características del ECG, ahora en relación con la tasa de variación cardiaca\n",
    "\n",
    "rmssd_V = []\n",
    "rmssd_II = []\n",
    "meanNN_V = []\n",
    "meanNN_II = []\n",
    "sdNN_V = []\n",
    "sdNN_II = []\n",
    "sdsd_V = []\n",
    "sdsd_II = []\n",
    "cvNN_V = []\n",
    "cvNN_II = []\n",
    "CVSD_V = []\n",
    "CVSD_II = []\n",
    "medianNN_V = [] \n",
    "medianNN_II = []\n",
    "madNN_V = []\n",
    "madNN_II = []\n",
    "mcvNN_V = []\n",
    "mcvNN_II = []\n",
    "pNN50_V = []\n",
    "pNN50_II = []\n",
    "pNN20_V = []\n",
    "pNN20_II = []\n",
    "for i in range(len(lista_filtrada_V)):\n",
    "    statistics = nk.hrv_time (rpeaks_V[i], sampling_rate = 125)\n",
    "    rmssd_V.append(statistics['HRV_RMSSD'][0])\n",
    "    meanNN_V.append(statistics['HRV_MeanNN'][0])\n",
    "    sdNN_V.append(statistics['HRV_SDNN'][0])\n",
    "    sdsd_V.append(statistics['HRV_SDSD'][0])\n",
    "    cvNN_V.append(statistics['HRV_CVNN'][0])\n",
    "    CVSD_V.append(statistics['HRV_CVSD'][0])\n",
    "    medianNN_V.append(statistics['HRV_MedianNN'][0])\n",
    "    madNN_V.append(statistics['HRV_MadNN'][0])\n",
    "    mcvNN_V.append(statistics['HRV_MCVNN'][0])\n",
    "    pNN50_V.append(statistics['HRV_pNN50'][0])\n",
    "    pNN20_V.append(statistics['HRV_pNN20'][0])\n",
    "for i in range(len(lista_filtrada_II)):\n",
    "    statistics = nk.hrv_time (rpeaks_II[i], sampling_rate = 125)\n",
    "    rmssd_II.append(statistics['HRV_RMSSD'][0])\n",
    "    meanNN_II.append(statistics['HRV_MeanNN'][0])\n",
    "    sdNN_II.append(statistics['HRV_SDNN'][0])\n",
    "    sdsd_II.append(statistics['HRV_SDSD'][0])\n",
    "    cvNN_II.append(statistics['HRV_CVNN'][0])\n",
    "    CVSD_II.append(statistics['HRV_CVSD'][0])\n",
    "    medianNN_II.append(statistics['HRV_MedianNN'][0])\n",
    "    madNN_II.append(statistics['HRV_MadNN'][0])\n",
    "    mcvNN_II.append(statistics['HRV_MCVNN'][0])\n",
    "    pNN50_II.append(statistics['HRV_pNN50'][0])\n",
    "    pNN20_II.append(statistics['HRV_pNN20'][0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos parámetros frecuenciales (potencias en rangos de frecuencia)\n",
    "lf_V = []\n",
    "hf_V = []\n",
    "vhf_V = []\n",
    "lfn_V = [] #normalizada\n",
    "hfn_V = [] #normalizada\n",
    "ratiolfhf_V = []\n",
    "lf_II = []\n",
    "hf_II = []\n",
    "vhf_II = []\n",
    "lfn_II = [] #normalizada\n",
    "hfn_II = [] #normalizada\n",
    "ratiolfhf_II = []\n",
    "param_frec = nk.hrv_frequency (rpeaks_V[40], sampling_rate = 125)\n",
    "for i in range(len(lista_filtrada_V)):\n",
    "    try:\n",
    "        param_frec = nk.hrv_frequency (rpeaks_V[i], sampling_rate = 125)\n",
    "        lf_V.append(param_frec['HRV_LF'][0])\n",
    "        hf_V.append(param_frec['HRV_HF'][0])\n",
    "        vhf_V.append(param_frec['HRV_VHF'][0])\n",
    "        ratiolfhf_V.append(param_frec['HRV_LFHF'][0])\n",
    "        lfn_V.append(param_frec['HRV_LFn'][0])\n",
    "        hfn_V.append(param_frec['HRV_HFn'][0])\n",
    "    except IndexError: #Salta para aquellas muestras a las que previamente les habíamos \n",
    "         #asignado un 0 152, 162, 164, 322, 324, 384, 387, 389, 498, 508\n",
    "        lf_V.append(0.06)\n",
    "        hf_V.append(0.3)\n",
    "        vhf_V.append(0.45)\n",
    "        ratiolfhf_V.append(0.2)\n",
    "        lfn_V.append(0.2)\n",
    "        hfn_V.append(0.3)\n",
    "    except ValueError: #No salta para todas las anteriores pero sí para algunas que \n",
    "        #cumplen lo ya dicho, la 162\n",
    "        lf_V.append(0.06)\n",
    "        hf_V.append(0.3)\n",
    "        vhf_V.append(0.45)\n",
    "        ratiolfhf_V.append(0.2)\n",
    "        lfn_V.append(0.2)\n",
    "        hfn_V.append(0.3)\n",
    "for i in range(len(lista_filtrada_II)):\n",
    "    try:\n",
    "        param_frec = nk.hrv_frequency (rpeaks_II[i], sampling_rate = 125)\n",
    "        lf_II.append(param_frec['HRV_LF'][0])\n",
    "        hf_II.append(param_frec['HRV_HF'][0])\n",
    "        vhf_II.append(param_frec['HRV_VHF'][0])\n",
    "        ratiolfhf_II.append(param_frec['HRV_LFHF'][0])\n",
    "        lfn_II.append(param_frec['HRV_LFn'][0])\n",
    "        hfn_II.append(param_frec['HRV_HFn'][0]) \n",
    "    except IndexError: #Salta para aquellas muestras a las que previamente les habíamos \n",
    "         #asignado un 0 152, 389\n",
    "        lf_II.append(0.06)\n",
    "        hf_II.append(0.3)\n",
    "        vhf_II.append(0.45)\n",
    "        ratiolfhf_II.append(0.2)\n",
    "        lfn_II.append(0.2)\n",
    "        hfn_II.append(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos la calidad de la señal\n",
    "quality_V = []\n",
    "quality_II = []\n",
    "#152, 162, 164, 322, 324, 384, 387, 389, 498, 508\n",
    "for i in range(len(lista_filtrada_V)):\n",
    "    try:\n",
    "        SQI = nk.ecg_quality(lista_filtrada_V[i], rpeaks_V[i], sampling_rate = 125)\n",
    "        quality_V.append(SQI[0])\n",
    "    except IndexError:\n",
    "        quality_V.append(0.7)\n",
    "    except ZeroDivisionError:\n",
    "        quality_V.append(0.7)\n",
    "    except ValueError:\n",
    "        quality_V.append(0.7)\n",
    "for i in range(len(lista_filtrada_II)): #152, 389\n",
    "    try:\n",
    "        SQI = nk.ecg_quality(lista_filtrada_II[i], rpeaks_II[i], sampling_rate = 125) \n",
    "        quality_II.append(SQI[0])\n",
    "    except IndexError:\n",
    "        quality_II.append(0.7)\n",
    "    except ZeroDivisionError:\n",
    "        quality_II.append(0.7)\n",
    "    except ValueError:\n",
    "        quality_II.append(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrv_V = []\n",
    "hrv_SD1_V = []\n",
    "hrv_SD2_V = []\n",
    "hrv_SD1SD2_V = []\n",
    "hrv_S_V = []\n",
    "hrv_CSI_V = []\n",
    "hrv_CVI_V = []\n",
    "hrv_CSI_Modified_V = []\n",
    "hrv_GI_V = []\n",
    "hrv_SI_V = []\n",
    "hrv_AI_V = []\n",
    "hrv_PI_V = []\n",
    "hrv_SD1d_V = []\n",
    "hrv_SD1a_V = []\n",
    "hrv_C1d_V = []\n",
    "hrv_C1a_V = []\n",
    "hrv_SD2d_V = []\n",
    "hrv_SD2a_V = []\n",
    "hrv_C2d_V = []\n",
    "hrv_C2a_V = []\n",
    "hrv_SDNNd_V = []\n",
    "hrv_SDNNa_V = []\n",
    "hrv_Cd_V = []\n",
    "hrv_Ca_V = []\n",
    "hrv_PIP_V = []\n",
    "hrv_IALS_V = []\n",
    "hrv_PSS_V = []\n",
    "hrv_PAS_V = []\n",
    "hrv_ApEn_V = []\n",
    "hrv_SampEn_V = []\n",
    "hrv_II = []\n",
    "hrv_SD1_II = []\n",
    "hrv_SD2_II = []\n",
    "hrv_SD1SD2_II = []\n",
    "hrv_S_II = []\n",
    "hrv_CSI_II = []\n",
    "hrv_CVI_II = []\n",
    "hrv_CSI_Modified_II = []\n",
    "hrv_GI_II = []\n",
    "hrv_SI_II = []\n",
    "hrv_AI_II = []\n",
    "hrv_PI_II = []\n",
    "hrv_SD1d_II = []\n",
    "hrv_SD1a_II = []\n",
    "hrv_C1d_II = []\n",
    "hrv_C1a_II = []\n",
    "hrv_SD2d_II = []\n",
    "hrv_SD2a_II = []\n",
    "hrv_C2d_II = []\n",
    "hrv_C2a_II = []\n",
    "hrv_SDNNd_II = []\n",
    "hrv_SDNNa_II = []\n",
    "hrv_Cd_II = []\n",
    "hrv_Ca_II = []\n",
    "hrv_PIP_II = []\n",
    "hrv_IALS_II = []\n",
    "hrv_PSS_II = []\n",
    "hrv_PAS_II = []\n",
    "hrv_ApEn_II = []\n",
    "hrv_SampEn_II = []\n",
    "for i in range(len(lista_filtrada_V)):\n",
    "    try:\n",
    "        hrv_V = nk.hrv_nonlinear(peaks = rpeaks_V[i] ,sampling_rate=125)\n",
    "        hrv_SD1_V.append(hrv['HRV_SD1'][0])\n",
    "        hrv_SD2_V.append(hrv['HRV_SD2'][0])\n",
    "        hrv_SD1SD2_V.append(hrv['HRV_SD1SD2'][0])\n",
    "        hrv_S_V.append(hrv['HRV_S'][0])\n",
    "        hrv_CSI_V.append(hrv['HRV_CSI'][0])\n",
    "        hrv_CVI_V.append(hrv['HRV_CVI'][0])\n",
    "        hrv_CSI_Modified_V.append(hrv['HRV_CSI_Modified'][0])\n",
    "        hrv_GI_V.append(hrv['HRV_GI'][0])\n",
    "        hrv_SI_V.append(hrv['HRV_SI'][0])\n",
    "        hrv_AI_V.append(hrv['HRV_AI'][0])\n",
    "        hrv_PI_V.append(hrv['HRV_PI'][0])\n",
    "        hrv_SD1d_V.append(hrv['HRV_SD1d'][0])\n",
    "        hrv_SD1a_V.append(hrv['HRV_SD1a'][0])\n",
    "        hrv_C1d_V .append(hrv['HRV_C1d'][0])\n",
    "        hrv_C1a_V.append(hrv['HRV_C1a'][0])\n",
    "        hrv_SD2d_V.append(hrv['HRV_SD2d'][0])\n",
    "        hrv_SD2a_V.append(hrv['HRV_SD2a'][0])\n",
    "        hrv_C2d_V.append(hrv['HRV_C2d'][0])\n",
    "        hrv_C2a_V.append(hrv['HRV_C2a'][0])\n",
    "        hrv_SDNNd_V.append(hrv['HRV_SDNNd'][0])\n",
    "        hrv_SDNNa_V.append(hrv['HRV_SDNNa'][0])\n",
    "        hrv_Cd_V.append(hrv['HRV_Cd'][0])\n",
    "        hrv_Ca_V.append(hrv['HRV_Ca'][0])\n",
    "        hrv_PIP_V.append(hrv['HRV_PIP'][0])\n",
    "        hrv_IALS_V.append(hrv['HRV_IALS'][0])\n",
    "        hrv_PSS_V.append(hrv['HRV_PSS'][0])\n",
    "        hrv_PAS_V.append(hrv['HRV_PAS'][0])\n",
    "        hrv_ApEn_V.append(hrv['HRV_ApEn'][0])\n",
    "        hrv_SampEn_V.append(hrv['HRV_SampEn'][0])\n",
    "    except: #ZeroDivisionError, ValueError\n",
    "        hrv_SD1_V.append(0.01)\n",
    "        hrv_SD2_V.append(0.01)\n",
    "        hrv_SD1SD2_V.append(0.01)\n",
    "        hrv_S_V.append(0.01)\n",
    "        hrv_CSI_V.append(0.01)\n",
    "        hrv_CVI_V.append(0.01)\n",
    "        hrv_CSI_Modified_V.append(0.01)\n",
    "        hrv_GI_V.append(0.01)\n",
    "        hrv_SI_V.append(0.01)\n",
    "        hrv_AI_V.append(0.01)\n",
    "        hrv_PI_V.append(0.01)\n",
    "        hrv_SD1d_V.append(0.01)\n",
    "        hrv_SD1a_V.append(0.01)\n",
    "        hrv_C1d_V .append(0.01)\n",
    "        hrv_C1a_V.append(0.01)\n",
    "        hrv_SD2d_V.append(0.01)\n",
    "        hrv_SD2a_V.append(0.01)\n",
    "        hrv_C2d_V.append(0.01)\n",
    "        hrv_C2a_V.append(0.01)\n",
    "        hrv_SDNNd_V.append(0.01)\n",
    "        hrv_SDNNa_V.append(0.01)\n",
    "        hrv_Cd_V.append(0.01)\n",
    "        hrv_Ca_V.append(0.01)\n",
    "        hrv_PIP_V.append(0.01)\n",
    "        hrv_IALS_V.append(0.01)\n",
    "        hrv_PSS_V.append(0.01)\n",
    "        hrv_PAS_V.append(0.01)\n",
    "        hrv_ApEn_V.append(0.01)\n",
    "        hrv_SampEn_V.append(0.01)\n",
    "for i in range(len(lista_filtrada_II)):\n",
    "    try:\n",
    "        hrv_II = nk.hrv_nonlinear(peaks = rpeaks_II[i], sampling_rate = 125)\n",
    "        hrv_SD1_II.append(hrv['HRV_SD1'][0])\n",
    "        hrv_SD2_II.append(hrv['HRV_SD2'][0])\n",
    "        hrv_SD1SD2_II.append(hrv['HRV_SD1SD2'][0])\n",
    "        hrv_S_II.append(hrv['HRV_S'][0])\n",
    "        hrv_CSI_II.append(hrv['HRV_CSI'][0])\n",
    "        hrv_CVI_II.append(hrv['HRV_CVI'][0])\n",
    "        hrv_CSI_Modified_II.append(hrv['HRV_CSI_Modified'][0])\n",
    "        hrv_GI_II.append(hrv['HRV_GI'][0])\n",
    "        hrv_SI_II.append(hrv['HRV_SI'][0])\n",
    "        hrv_AI_II.append(hrv['HRV_AI'][0])\n",
    "        hrv_PI_II.append(hrv['HRV_PI'][0])\n",
    "        hrv_SD1d_II.append(hrv['HRV_SD1d'][0])\n",
    "        hrv_SD1a_II.append(hrv['HRV_SD1a'][0])\n",
    "        hrv_C1d_II.append(hrv['HRV_C1d'][0])\n",
    "        hrv_C1a_II.append(hrv['HRV_C1a'][0])\n",
    "        hrv_SD2d_II.append(hrv['HRV_SD2d'][0])\n",
    "        hrv_SD2a_II.append(hrv['HRV_SD2a'][0])\n",
    "        hrv_C2d_II.append(hrv['HRV_C2d'][0])\n",
    "        hrv_C2a_II.append(hrv['HRV_C2a'][0])\n",
    "        hrv_SDNNd_II.append(hrv['HRV_SDNNd'][0])\n",
    "        hrv_SDNNa_II.append(hrv['HRV_SDNNa'][0])\n",
    "        hrv_Cd_II.append(hrv['HRV_Cd'][0])\n",
    "        hrv_Ca_II.append(hrv['HRV_Ca'][0])\n",
    "        hrv_PIP_II.append(hrv['HRV_PIP'][0])\n",
    "        hrv_IALS_II.append(hrv['HRV_IALS'][0])\n",
    "        hrv_PSS_II.append(hrv['HRV_PSS'][0])\n",
    "        hrv_PAS_II.append(hrv['HRV_PAS'][0])\n",
    "        hrv_ApEn_II.append(hrv['HRV_ApEn'][0])\n",
    "        hrv_SampEn_II.append(hrv['HRV_SampEn'][0])\n",
    "    except: #ValueError, ZeroDivisionError\n",
    "        hrv_SD1_II.append(0.01)\n",
    "        hrv_SD2_II.append(0.01)\n",
    "        hrv_SD1SD2_II.append(0.01)\n",
    "        hrv_S_II.append(0.01)\n",
    "        hrv_CSI_II.append(0.01)\n",
    "        hrv_CVI_II.append(0.01)\n",
    "        hrv_CSI_Modified_II.append(0.01)\n",
    "        hrv_GI_II.append(0.01)\n",
    "        hrv_SI_II.append(0.01)\n",
    "        hrv_AI_II.append(0.01)\n",
    "        hrv_PI_II.append(0.01)\n",
    "        hrv_SD1d_II.append(0.01)\n",
    "        hrv_SD1a_II.append(0.01)\n",
    "        hrv_C1d_II.append(0.01)\n",
    "        hrv_C1a_II.append(0.01)\n",
    "        hrv_SD2d_II.append(0.01)\n",
    "        hrv_SD2a_II.append(0.01)\n",
    "        hrv_C2d_II.append(0.01)\n",
    "        hrv_C2a_II.append(0.01)\n",
    "        hrv_SDNNd_II.append(0.01)\n",
    "        hrv_SDNNa_II.append(0.01)\n",
    "        hrv_Cd_II.append(0.01)\n",
    "        hrv_Ca_II.append(0.01)\n",
    "        hrv_PIP_II.append(0.01)\n",
    "        hrv_IALS_II.append(0.01)\n",
    "        hrv_PSS_II.append(0.01)\n",
    "        hrv_PAS_II.append(0.01)\n",
    "        hrv_ApEn_II.append(0.01)\n",
    "        hrv_SampEn_II.append(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos rr intervals maximos y minimos\n",
    "max_int_RR_II = []\n",
    "max_int_RR_V = []\n",
    "for i in range(len(rr_interval_V)):\n",
    "    elem = max(rr_interval_V[i])\n",
    "    max_int_RR_V.append(elem)\n",
    "for i in range(len(rr_interval_II)):\n",
    "    elem = max(rr_interval_II[i])\n",
    "    max_int_RR_II.append(elem)\n",
    "#Calculamos estadisticos del heart rate\n",
    "media_hrV = []\n",
    "minimo_hrV = []\n",
    "maximo_hrV = []\n",
    "desviacion_hrV = []\n",
    "media_hrII = []\n",
    "minimo_hrII = []\n",
    "maximo_hrII = []\n",
    "desviacion_hrII = []\n",
    "for i in range(len(rr_interval_V)):\n",
    "    try:\n",
    "        estadisticos_hr = pyhrv.time_domain.hr_parameters(nni = rr_interval_V[i])\n",
    "        media_hrV.append(estadisticos_hr[0])\n",
    "        minimo_hrV.append(estadisticos_hr[1])\n",
    "        maximo_hrV.append(estadisticos_hr[2])\n",
    "        desviacion_hrV.append(estadisticos_hr[3])\n",
    "    except: #152, 164, 322, 324, 384, 387, 389, 498, 508, IndexError, ValueError\n",
    "        media_hrV.append(45)\n",
    "        minimo_hrV.append(25)\n",
    "        maximo_hrV.append(51)\n",
    "        desviacion_hrV.append(2)\n",
    "for i in range(len(rr_interval_II)):\n",
    "    try:\n",
    "        estadisticos_hr = pyhrv.time_domain.hr_parameters(nni = rr_interval_II[i])\n",
    "        media_hrII.append(estadisticos_hr[0])\n",
    "        minimo_hrII.append(estadisticos_hr[1])\n",
    "        maximo_hrII.append(estadisticos_hr[2])\n",
    "        desviacion_hrII.append(estadisticos_hr[3])\n",
    "    except ValueError:#153, 389\n",
    "        media_hrII.append(45)\n",
    "        minimo_hrII.append(25)\n",
    "        maximo_hrII.append(51)\n",
    "        desviacion_hrII.append(2)\n",
    "#Calculamos diferencias entre intervalos NN (picos R normalizados)\n",
    "media_nni_dif_II = []\n",
    "min_nni_dif_II = []\n",
    "max_nni_dif_II = []\n",
    "media_nni_dif_V = []\n",
    "min_nni_dif_V = []\n",
    "max_nni_dif_V = []\n",
    "for i in range(len(rr_interval_V)):\n",
    "    try:\n",
    "        nni_differences = pyhrv.time_domain.nni_differences_parameters(nni = rr_interval_II[i][0])\n",
    "        media_nni_dif_V.append(nni_differences['nni_diff_mean'])\n",
    "        min_nni_dif_V.append(nni_differences['nni_diff_min'])\n",
    "        max_nni_dif_V.append(nni_differences['nni_diff_max'])\n",
    "    except IndexError:#152, 164, 322, 324, 384, 387, 389, 498, 508\n",
    "        media_nni_dif_V.append(282)\n",
    "        min_nni_dif_V.append(0)\n",
    "        max_hrV.append(1248)\n",
    "\n",
    "    except ValueError:#153, 389\n",
    "        media_nni_dif_V.append(282)\n",
    "        min_nni_dif_V.append(0)\n",
    "        max_nni_dif_V.append(1248)\n",
    "\n",
    "for i in range(len(rr_interval_II)):\n",
    "    try:\n",
    "        nni_differences = pyhrv.time_domain.nni_differences_parameters(nni = rr_interval_II[i][0])\n",
    "        media_nni_dif_II.append(nni_differences['nni_diff_mean'])\n",
    "        min_nni_dif_II.append(nni_differences['nni_diff_min'])\n",
    "        max_nni_dif_II.append(nni_differences['nni_diff_max'])\n",
    "    except ValueError:#153, 389\n",
    "        media_nni_dif_II.append(282)\n",
    "        min_nni_dif_II.append(0)\n",
    "        max_nni_dif_II.append(1248)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "index = []\n",
    "for i in range(len(lista_filtrada_II)):\n",
    "    index.append(i)\n",
    "d = {'index': index, 'col1': media_hrII, 'col2': desviacion_hrII}\n",
    "X = pd.DataFrame(data = d) \n",
    "X = np.array(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "index = []\n",
    "for i in range(len(lista_filtrada_II)):\n",
    "    index.append(i)\n",
    "d = {'index': index, 'col1': media_hrII, 'col2': desviacion_hrII, 'col3': hrv_SD1SD2_II, \n",
    "    'col4' : hrv_SD1SD2_V, 'col5': media_hrV, 'col6': desviacion_hrV, 'col7': hrv_SampEn_II, \n",
    "    'col8': hrv_SampEn_V}\n",
    "X = pd.DataFrame(data = d) \n",
    "X = np.array(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get labels for the patients\n",
    "y = np.loadtxt('/kaggle/input/physiological-signals-processing-challenge-2021/alarms_training.csv',skiprows=1,delimiter = ',',usecols = [0,1])\n",
    "\n",
    "y_train = y[np.array(X[:,0]-1,dtype = int),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#get priors\n",
    "\n",
    "P_h0 = np.mean(y_train[:,1] == 0)\n",
    "P_h1 = np.mean(y_train[:,1] == 1)\n",
    "\n",
    "print('P_h0:',P_h0,'; P_h1:',P_h1)\n",
    "\n",
    "#naive bayes model\n",
    "\n",
    "nb_detector = GaussianNB(priors = [P_h0,P_h1])\n",
    "\n",
    "#train the model\n",
    "nb_detector.fit(X[:,1:],y_train[:,1])\n",
    "\n",
    "print(\"mean values n_classes, n_features\")\n",
    "print(nb_detector.theta_)\n",
    "\n",
    "print(\"variance values n_classes, n_features\")\n",
    "print(nb_detector.sigma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict first case\n",
    "\n",
    "x_1 = X[0,1:]\n",
    "\n",
    "D = nb_detector.predict(x_1[np.newaxis,:])\n",
    "\n",
    "print(X[0,0])\n",
    "print(y_train[0,0])\n",
    "print('Detection: %d'%D[0])\n",
    "print('Hypothesis H: %d' %y_train[0,1])\n",
    "\n",
    "#whole training set\n",
    "y_hat = nb_detector.predict(X[:,1:])\n",
    "\n",
    "\n",
    "print('ACC = %.2f'%np.mean(y_hat == y_train[:,1]))\n",
    "from sklearn.metrics import f1_score\n",
    "print('f1 =', f1_score(y_train[:, 1], y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \n",
    "\n",
    "X_test = []\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/physiological-signals-processing-challenge-2021/data_challenge/test/'):\n",
    "    filenames.sort()\n",
    "    j = 0\n",
    "    for filename in filenames:\n",
    "        #print(filename)\n",
    "        \n",
    "        if j%25==0:\n",
    "            clear_output(wait = True)\n",
    "            print(j,\" of \",len(filenames)/2)\n",
    "            #print(filename)\n",
    "            \n",
    "        if 'mat' in filename:\n",
    "            continue\n",
    "        #\n",
    "        j+=1\n",
    "        #read the pat data\n",
    "        record = loadmat(os.path.join(dirname,filename[:-4]+'.mat'))\n",
    "        \n",
    "        #get the header\n",
    "        with open(os.path.join(dirname,filename)) as f:\n",
    "                header = f.readlines()\n",
    "\n",
    "        #pass to the function to get the mean and std heart rate\n",
    "        \n",
    "        mean_hr,std_hr = get_mean_std_hr_one_pat(record,header,plot_flag = False)\n",
    "        \n",
    "        X_test.append([int(filename[:-4]),mean_hr,std_hr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort data\n",
    "X_test = np.array(X_test)\n",
    "print(X_test[:3,:])\n",
    "idx_sort = np.argsort(X_test[:,0])\n",
    "\n",
    "X_test = X_test[idx_sort,:]\n",
    "\n",
    "print(X_test[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "\n",
    "y_hat_test = nb_detector.predict(X_test[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_hat_test[:])\n",
    "\n",
    "#create solution\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Id': X_test[:,0], 'Category': y_hat_test})\n",
    "\n",
    "df.to_csv('submission_naive_bayes.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
